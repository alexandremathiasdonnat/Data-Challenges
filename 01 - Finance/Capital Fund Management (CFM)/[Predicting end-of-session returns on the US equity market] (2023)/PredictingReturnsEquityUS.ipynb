{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8959e07a",
   "metadata": {},
   "source": [
    "<p align=\"center\">\n",
    "    <img src=\"https://upload.wikimedia.org/wikipedia/commons/7/74/Logo_%C3%89cole_normale_sup%C3%A9rieure_-_PSL_%28ENS-PSL%29.svg\"\n",
    "             alt=\"ENS-PSL\"\n",
    "             width=\"475\"\n",
    "             style=\"margin-right: 30px; display: inline-block; vertical-align: middle;\"/>\n",
    "    <img src=\"https://challengedata.ens.fr/logo/public/CFM_CoRGB_300dpi_Tight_box_Er2kNvB.png\"\n",
    "             alt=\"Crédit Agricole Assurances\"\n",
    "             width=\"260\"\n",
    "             style=\"display: inline-block; vertical-align: middle;\"/>\n",
    "</p>\n",
    "\n",
    "# Capital Fund Management - Intraday Price Path Classification\n",
    "**Predicting Afternoon (14h - 16h) Price Direction from Morning Intraday Trajectories**\n",
    "\n",
    "## Data Challenge \n",
    "**Powered by ENS** \n",
    "\n",
    "<h3><span style=\"color:#800000;\"><strong>Authored by:</strong> <em>Alexandre Mathias DONNAT, Sr</em></span></h3>\n",
    "\n",
    "**Curently ranked 139/260** on *https://challengedata.ens.fr/challenges/84*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "751fbfbc",
   "metadata": {},
   "source": [
    "This notebook presents a gradient-boosting framework for predicting afternoon price direction from morning intraday paths of anonymous equities.\n",
    "\n",
    "The objective is to classify each 09:30–14:00 price trajectory into one of three classes (strong down / flat / strong up), using only the 53 five-minute returns observed before 14:00.\n",
    "\n",
    "The dataset is unusually large for a tabular intraday challenge: both the training set and the test set contain nearly 850,000 rows, each corresponding to a different (day, equity) scenario. Memory efficiency and feature-engineering discipline are therefore essential.\n",
    "\n",
    "Each training sample consists of:\n",
    "\n",
    "- A unique (day, equity) pair — both anonymized, and never overlapping between train and test,\n",
    "- A sequence of 53 intraday returns **r0…r52** in basis points, describing the price path from 09:30 to 14:00,\n",
    "- A target label **reod** ∈ {−1, 0, 1} encoding the direction and magnitude of the subsequent 14:00–16:00 return:\n",
    "    - **−1** = strong negative move (< −25 bps)\n",
    "    - **0** = small / flat move (between −25 and +25 bps)\n",
    "    - **+1** = strong positive move (> +25 bps)\n",
    "\n",
    "The challenge is therefore to infer the future two-hour movement using only the shape, volatility, and micro-structure patterns of the morning trajectory.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5429074e",
   "metadata": {},
   "source": [
    "## Understanding the modeling problem\n",
    "\n",
    "Each row (each ID) corresponds to:\n",
    "\n",
    "- One stock on one trading day,\n",
    "- A contiguous intraday path of 53 five-minute returns from 09:30 to 14:00,\n",
    "- A 3-class label describing how that same stock moved between 14:00 and 16:00.\n",
    "\n",
    "**Our task is:**\n",
    "\n",
    "Given the 09:30–14:00 return path, predict whether the 14:00–16:00 move will be strongly down, flat, or strongly up.\n",
    "\n",
    "Several factors make this prediction intrinsically difficult:\n",
    "\n",
    "- Afternoon returns are small, noisy, and weakly autocorrelated.\n",
    "- Crucial price drivers (news releases, liquidity shocks, order-book imbalances) are absent from the dataset.\n",
    "- The competition enforces strict generalisation:\n",
    "    - train and test contain different days,\n",
    "    - train and test contain different equities,\n",
    "    - → the model cannot memorize anything — it must capture generic intraday patterns only.\n",
    "\n",
    "The signal-to-noise ratio is extremely low, meaning even strong models typically peak around 40–42% accuracy.\n",
    "\n",
    "Yet, intraday trajectories still carry exploitable statistical fingerprints:\n",
    "\n",
    "- **Directional imbalance** (`ret_sum`, net bias of positive vs negative returns),\n",
    "- **Intraday volatility profile** (`ret_std`, count of large moves),\n",
    "- **Shock frequency** (`big_move_count_25` / `50`),\n",
    "- Cases where strong morning trends extend vs mean-revert in the afternoon.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5fdf82d7",
   "metadata": {},
   "source": [
    "## Data description\n",
    "\n",
    "Three CSV files are provided:\n",
    "\n",
    "#### 1) `x_train.csv` – Intraday features (inputs)\n",
    "\n",
    "Each row corresponds to one (day, equity) observation, uniquely identified by an ID. Main columns:\n",
    "\n",
    "- **ID** – unique row identifier,\n",
    "- **day** – day identifier (no overlap between train and test),\n",
    "- **equity** – stock identifier (also disjoint between train and test),\n",
    "- **r0 … r52** – 53 intraday returns in basis points:\n",
    "\n",
    "$$r_t = \\frac{P_{t+5\\text{min}} - P_t}{P_t} \\times 10^4$$\n",
    "\n",
    "These 53 returns encode the morning price trajectory from 09:30 to 14:00 for each stock and each day.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e69b7ab3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ID</th>\n",
       "      <th>day</th>\n",
       "      <th>equity</th>\n",
       "      <th>r0</th>\n",
       "      <th>r1</th>\n",
       "      <th>r2</th>\n",
       "      <th>r3</th>\n",
       "      <th>r4</th>\n",
       "      <th>r5</th>\n",
       "      <th>r6</th>\n",
       "      <th>...</th>\n",
       "      <th>r43</th>\n",
       "      <th>r44</th>\n",
       "      <th>r45</th>\n",
       "      <th>r46</th>\n",
       "      <th>r47</th>\n",
       "      <th>r48</th>\n",
       "      <th>r49</th>\n",
       "      <th>r50</th>\n",
       "      <th>r51</th>\n",
       "      <th>r52</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>249</td>\n",
       "      <td>1488</td>\n",
       "      <td>0.00</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.00</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.00</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.00</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>272</td>\n",
       "      <td>107</td>\n",
       "      <td>-9.76</td>\n",
       "      <td>0.00</td>\n",
       "      <td>-12.21</td>\n",
       "      <td>46.44</td>\n",
       "      <td>34.08</td>\n",
       "      <td>0.00</td>\n",
       "      <td>41.24</td>\n",
       "      <td>...</td>\n",
       "      <td>-4.83</td>\n",
       "      <td>-16.92</td>\n",
       "      <td>-4.84</td>\n",
       "      <td>4.84</td>\n",
       "      <td>0.00</td>\n",
       "      <td>7.26</td>\n",
       "      <td>-9.68</td>\n",
       "      <td>-19.38</td>\n",
       "      <td>9.71</td>\n",
       "      <td>26.68</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>323</td>\n",
       "      <td>1063</td>\n",
       "      <td>49.85</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>-26.64</td>\n",
       "      <td>-23.66</td>\n",
       "      <td>-22.14</td>\n",
       "      <td>49.12</td>\n",
       "      <td>...</td>\n",
       "      <td>-6.37</td>\n",
       "      <td>1.59</td>\n",
       "      <td>6.37</td>\n",
       "      <td>-49.32</td>\n",
       "      <td>-9.59</td>\n",
       "      <td>-6.40</td>\n",
       "      <td>22.41</td>\n",
       "      <td>-6.39</td>\n",
       "      <td>7.99</td>\n",
       "      <td>15.96</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>302</td>\n",
       "      <td>513</td>\n",
       "      <td>0.00</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.00</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>123</td>\n",
       "      <td>1465</td>\n",
       "      <td>-123.84</td>\n",
       "      <td>-115.18</td>\n",
       "      <td>-26.44</td>\n",
       "      <td>0.00</td>\n",
       "      <td>42.42</td>\n",
       "      <td>10.56</td>\n",
       "      <td>0.00</td>\n",
       "      <td>...</td>\n",
       "      <td>-5.36</td>\n",
       "      <td>-21.44</td>\n",
       "      <td>-21.48</td>\n",
       "      <td>10.78</td>\n",
       "      <td>-21.55</td>\n",
       "      <td>-5.40</td>\n",
       "      <td>-10.81</td>\n",
       "      <td>5.41</td>\n",
       "      <td>-32.47</td>\n",
       "      <td>43.43</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 56 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   ID  day  equity      r0      r1     r2     r3     r4     r5     r6  ...  \\\n",
       "0   0  249    1488    0.00     NaN    NaN    NaN   0.00    NaN    NaN  ...   \n",
       "1   1  272     107   -9.76    0.00 -12.21  46.44  34.08   0.00  41.24  ...   \n",
       "2   2  323    1063   49.85    0.00   0.00 -26.64 -23.66 -22.14  49.12  ...   \n",
       "3   3  302     513    0.00     NaN   0.00   0.00   0.00    NaN    NaN  ...   \n",
       "4   4  123    1465 -123.84 -115.18 -26.44   0.00  42.42  10.56   0.00  ...   \n",
       "\n",
       "    r43    r44    r45    r46    r47   r48    r49    r50    r51    r52  \n",
       "0  0.00   0.00    NaN   0.00    NaN  0.00    NaN    NaN    NaN   0.00  \n",
       "1 -4.83 -16.92  -4.84   4.84   0.00  7.26  -9.68 -19.38   9.71  26.68  \n",
       "2 -6.37   1.59   6.37 -49.32  -9.59 -6.40  22.41  -6.39   7.99  15.96  \n",
       "3   NaN    NaN    NaN    NaN    NaN   NaN    NaN    NaN   0.00    NaN  \n",
       "4 -5.36 -21.44 -21.48  10.78 -21.55 -5.40 -10.81   5.41 -32.47  43.43  \n",
       "\n",
       "[5 rows x 56 columns]"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "df = pd.read_csv(\"x_train.csv\")\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42ec516c",
   "metadata": {},
   "source": [
    "#### 2) `y_train.csv` – Target labels\n",
    "\n",
    "For each ID in x_train.csv, this file provides the target:\n",
    "\n",
    "- **ID** – matches the same ID as in x_train.csv,\n",
    "- **reod** – integer in {−1, 0, 1} representing the direction of the 14:00–16:00 move:\n",
    "    - **−1** = strong downward move (less than −25 bps),\n",
    "    - **0** = flat / small move (between −25 and +25 bps),\n",
    "    - **+1** = strong upward move (more than +25 bps).\n",
    "\n",
    "This is the ground truth the model tries to predict."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "cc729ff3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ID</th>\n",
       "      <th>reod</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   ID  reod\n",
       "0   0     0\n",
       "1   1     0\n",
       "2   2    -1\n",
       "3   3     0\n",
       "4   4    -1"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y = pd.read_csv(\"y_train.csv\")\n",
    "y.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36b78353",
   "metadata": {},
   "source": [
    "#### 3) `x_test.csv` – Test set (unlabeled)\n",
    "\n",
    "Same structure as x_train.csv:\n",
    "\n",
    "- ID, day, equity, r0 … r52,\n",
    "\n",
    "Our final submission `y_prediction.csv` must contain:\n",
    "\n",
    "- **ID**,\n",
    "- **reod** – predicted class in {−1, 0, 1}."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c00a237",
   "metadata": {},
   "source": [
    "## 3 Evaluation metric and benchmark\n",
    "\n",
    "The platform evaluates submissions using the classification accuracy on the hidden test set.\n",
    "\n",
    "- A naive random or constant prediction yields ≈ 33% accuracy (3 classes).\n",
    "- The official benchmark pipeline reaches about 41.74% accuracy.\n",
    "\n",
    "The objective of this notebook is not to aggressively tune every detail under heavy compute, but to build a clean, robust and interpretable pipeline that:\n",
    "\n",
    "- uses only the intraday returns,\n",
    "- respects realistic constraints (8 GB RAM, no extreme feature explosion),\n",
    "- achieves a stable performance in the 0.40–0.42 accuracy range, i.e. competitive with the benchmark and proving that a signal is catched\n",
    "\n",
    "## Modelling philosophy\n",
    "\n",
    "Predicting the sign of a small afternoon move from a noisy morning trajectory is more tricky that it seems:\n",
    "\n",
    "- the signal-to-noise ratio is low,\n",
    "- markets can move on news (jumps) that are not observable in our features,\n",
    "- train and test periods have different days and different stocks, so overfitting on IDs is useless.\n",
    "\n",
    "Given these constraints, we adopt a minimalist but principled approach:\n",
    "\n",
    "- Use the full intraday path (r0…r52) as raw features, so the model can exploit any pattern in the shape of the trajectory.\n",
    "- Add only a small set of global statistics that summarize momentum and volatility, without exceeding my computer 8 Go RAM.\n",
    "- Deliberately exclude day and equity from the feature set to avoid modeling specific days/stocks that do not appear in the test set (no leakage-like behavior).\n",
    "- Train a single LightGBM multiclass model, carefully regularised, rather than stacking many heavy models that would not significantly beat the noise given the hardware and time budget.\n",
    "\n",
    "This pipeline is deliberately simple, stable and explainable. It also reflects a realistic trade-off between:\n",
    "\n",
    "- model complexity,\n",
    "- feature engineering effort,\n",
    "- computational resources.\n",
    "\n",
    "## Feature engineering strategy\n",
    "\n",
    "### Raw intraday path\n",
    "\n",
    "The core of the feature space is simply the 53 intraday returns:\n",
    "\n",
    "- r0, r1, …, r52.\n",
    "\n",
    "They form a discrete trajectory of the price movement from 09:30 to 14:00.\n",
    "\n",
    "We keep these columns as-is, after:\n",
    "\n",
    "- casting them to float,\n",
    "- replacing any missing values with 0.0.\n",
    "\n",
    "This ensures the model sees the full temporal structure of the morning move, without any arbitrary aggregation that might destroy signal.\n",
    "\n",
    "### Global trajectory statistics\n",
    "\n",
    "On top of the raw path, we add a few global summary statistics that capture:\n",
    "\n",
    "- net direction,\n",
    "- volatility,\n",
    "- asymmetry of moves.\n",
    "\n",
    "Concretely, for each row:\n",
    "\n",
    "- **ret_sum** – sum of all 53 returns: overall morning performance,\n",
    "- **ret_std** – standard deviation of the 53 returns: intraday volatility,\n",
    "- **pos_count** – number of positive 5-min returns,\n",
    "- **neg_count** – number of negative 5-min returns,\n",
    "- **big_move_count_25** – number of 5-min moves with |return| > 25 bps,\n",
    "- **big_move_count_50** – number of moves with |return| > 50 bps.\n",
    "\n",
    "**Idea behind:**\n",
    "\n",
    "- ret_sum and pos_count vs neg_count encode whether the morning was globally bullish or bearish.\n",
    "- ret_std and the big_move_count_* features measure how turbulent the session was. A noisy morning may correlate with larger afternoon moves.\n",
    "\n",
    "Together, the feature set is:\n",
    "\n",
    "- 53 raw returns,\n",
    "- 6 summary stats,\n",
    "- → **total 59 features** per sample (minus any missing columns, handled robustly).\n",
    "\n",
    "### Why we explicitly drop day and equity\n",
    "\n",
    "The challenge is constructed so that:\n",
    "\n",
    "- no day appears in both train and test,\n",
    "- no equity appears in both train and test.\n",
    "\n",
    "Using day or equity as features risks learning idiosyncratic patterns specific to some stocks or dates that will never generalize to the unseen test period.\n",
    "\n",
    "In practice, I observed that including these identifiers tends to inflate validation performance while having very limited value on the public leaderboard final accuracy. For this clean baseline, I therefore exclude them from the feature set and focus only on what is likely to generalize: the shape and volatility of the intraday path itself.\n",
    "\n",
    "## Modelling with LightGBM\n",
    "\n",
    "### Target encoding\n",
    "\n",
    "The target in y_train.csv is reod ∈ {−1, 0, 1}.\n",
    "\n",
    "LightGBM expects class indices in {0, 1, 2} for multiclass objectives.\n",
    "\n",
    "We therefore apply a deterministic mapping:\n",
    "\n",
    "- -1 → 0,\n",
    "- 0 → 1,\n",
    "- +1 → 2.\n",
    "\n",
    "And we store the inverse mapping to reconstruct the original labels before exporting the submission.\n",
    "\n",
    "### Train/validation split\n",
    "\n",
    "To estimate generalisation performance, we perform a stratified train/validation split:\n",
    "\n",
    "- 80% of the data for training,\n",
    "- 20% for validation,\n",
    "- stratification on the mapped target {0,1,2}.\n",
    "\n",
    "This preserves the class distribution in both folds and gives a reasonably stable estimate of validation accuracy and F1-macro.\n",
    "\n",
    "### LightGBM hyperparameters\n",
    "\n",
    "We use LightGBM in multiclass mode with the following configuration:\n",
    "\n",
    "- **objective** = \"multiclass\",\n",
    "- **num_class** = 3,\n",
    "- **learning_rate** = 0.05,\n",
    "- **num_leaves** = 31 : relatively small trees, more regular,\n",
    "- **max_depth** = -1  : let the model decide depth within the limit of num_leaves,\n",
    "- **feature_fraction** = 0.9 : random feature subsampling per tree, improves robustness,\n",
    "- **bagging_fraction** = 0.8, **bagging_freq** = 1 : row subsampling per iteration,\n",
    "- **min_data_in_leaf** = 200 : large leaves, strong regularisation against overfitting,\n",
    "- **lambda_l2** = 2.0 : L2 regularisation on leaf weights,\n",
    "- **metric** = [\"multi_logloss\", \"multi_error\"]  : monitor both loss and accuracy during training,\n",
    "- **n_estimators** = 2000,\n",
    "- **random_state** = 42,\n",
    "- **n_jobs** = -1.\n",
    "\n",
    "These parameters are intentionally conservative:\n",
    "\n",
    "- large min_data_in_leaf and non-trivial lambda_l2 bias the model towards smoother decision boundaries,\n",
    "- feature_fraction and bagging_fraction act like a built-in ensemble, reducing variance.\n",
    "\n",
    "Given the noisy nature of the problem and limited compute, we prefer a slightly underfit but stable model to a highly tuned, brittle one.\n",
    "\n",
    "### Early stopping and internal metrics\n",
    "\n",
    "Then we train the model with:\n",
    "\n",
    "- up to 2000 boosting iterations,\n",
    "- early stopping with a patience of 200 rounds using the validation logloss.\n",
    "\n",
    "At the end of training, it compute:\n",
    "\n",
    "- Validation accuracy (multi_error),\n",
    "- F1-macro (symmetric across the three classes),\n",
    "- Full classification report (precision/recall/F1 per class).\n",
    "\n",
    "Typical internal performance for this pipeline is around:\n",
    "\n",
    "- Accuracy ≈ 0.49,\n",
    "- F1-macro ≈ 0.48,\n",
    "\n",
    "which is consistent with the difficulty of the task and with public leaderboard scores around 0.41.\n",
    "\n",
    "## Full training and submission generation\n",
    "\n",
    "Once the best number of iterations `best_iter` is found on the validation split, we retrain a final LightGBM model on the entire training set:\n",
    "\n",
    "- same hyperparameters as above,\n",
    "- n_estimators = best_iter.\n",
    "\n",
    "This final model is then used to predict class probabilities on x_test.csv, and we:\n",
    "\n",
    "- take the argmax to obtain class indices {0,1,2},\n",
    "- map them back to the original classes {−1, 0, 1},\n",
    "- build the submission file:\n",
    "    - ID copied from x_test.csv,\n",
    "    - reod = predicted class.\n",
    "\n",
    "The notebook finishes by exporting:\n",
    "\n",
    "- **y_prediction.csv** (no index),\n",
    "- ready to be uploaded to the platform.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3bbe060",
   "metadata": {},
   "source": [
    "## Limitations and possible (few) improvements\n",
    "\n",
    "This notebook intentionally remains a clean, interpretable baseline.  \n",
    "Beyond this point, any improvement would rely less on modeling insight and more on brute-force search.\n",
    "\n",
    "### Why tuning the model further has limited value ?\n",
    "\n",
    "Further LGBM hyperparameter tuning (`num_leaves`, `learning_rate`, `max_depth`, regularization…) would not discover new structure.  \n",
    "It would only perturb LightGBM's partitioning of the 53-dimensional space.\n",
    "\n",
    "Yes, such random perturbations may, by chance, push accuracy from ~0.41 to ~0.45 or slightly higher, but:\n",
    "\n",
    "- each attempt requires long training times,\n",
    "- gains are unstable and non-generalizable,\n",
    "- improvements reflect noise exploitation rather than true modeling progress.\n",
    "\n",
    "This becomes a lottery, not an insight-driven process.\n",
    "\n",
    "### Why richer feature engineering was not pursued here ?\n",
    "\n",
    "The only genuine path to higher accuracy is **massive feature generation**.  \n",
    "But doing this properly requires hundreds or thousands of candidate features, which is incompatible with an 8 GB RAM budget unless the computations are distributed.\n",
    "\n",
    "More advanced pipelines would involve:\n",
    "\n",
    "- PCA or autoencoders on return paths,\n",
    "- rolling-window micro-pattern detectors,\n",
    "- volatility-regime segmentation,\n",
    "- cross-statistic interactions (ratios, differences, products),\n",
    "- and even random symbolic transformations such as:\n",
    "\n",
    "    ```\n",
    "    meta_1 = log(|r17| + 7) × cos(r3 − r28)\n",
    "    ```\n",
    "\n",
    "These transformations have no theoretical meaning but sometimes unlock tiny dataset-specific gains.\n",
    "\n",
    "**The problem:**  \n",
    "those gains do not generalize at all.  \n",
    "They only overfit the quirks of this specific context (specific given data, evaluated in this specific way).\n",
    "\n",
    "Going down that road would require:\n",
    "\n",
    "- either distributing computation on multiple VMs,\n",
    "- or implementing a full batch/streaming FE pipeline,\n",
    "\n",
    "which turns the challenge into a **systems engineering exercise**, not a modeling one.\n",
    "\n",
    "This was deliberately avoided.\n",
    "\n",
    "### The real takeaway (according to me)\n",
    "\n",
    "A multiclass accuracy **> 0.41** (compared to the 0.33 random baseline) already demonstrates something fundamental:\n",
    "\n",
    "> **The morning trajectory does contain real predictive signal — and the model is extracting it.**\n",
    "\n",
    "This is non-trivial.  \n",
    "In financial time series, theory and empirical evidence both agree:\n",
    "\n",
    "- Every price path contains a **predictable component** (microstructure biases, intraday seasonality, volatility clustering).\n",
    "- And an **irreducible stochastic component**, which cannot be forecast from past returns alone.\n",
    "\n",
    "In such a regime:\n",
    "\n",
    "- accuracy > 0.33 means the model is consistently beating randomness,\n",
    "- accuracy ≈ 0.4–0.45 indicates that some structure is exploitable,\n",
    "- accuracy > 0.5 would likely reflect dataset-specific artifacts, not a breakthrough in predictability.\n",
    "\n",
    "**Financial trajectories are partly orderly, partly random.**  \n",
    "The objective was not to predict perfectly, but to extract the fraction of orderliness that exists. We achieve that without overfitting or relying on speculative feature explosions.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da4cfd56",
   "metadata": {},
   "source": [
    "# Imports & data loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8bdc911",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shapes:\n",
      "  X_train: (843299, 56)\n",
      "  y_train: (843299, 2)\n",
      "  X_test : (885799, 56)\n",
      "\n",
      "Head X_train:\n",
      "   ID  day  equity     r0   r1     r2     r3     r4     r5     r6     r7     r8     r9    r10    r11   r12    r13   r14    r15    r16   r17    r18    r19    r20    r21    r22    r23    r24     r25  \\\n",
      "0   0  249    1488   0.00  NaN    NaN    NaN   0.00    NaN    NaN -68.03 -34.25    NaN    NaN    NaN  0.00    NaN   NaN   0.00    NaN   NaN    NaN    NaN   0.00   0.00   0.00    NaN   0.00  137.93   \n",
      "1   1  272     107  -9.76  0.0 -12.21  46.44  34.08   0.00  41.24  12.08 -26.54  19.32  48.22  23.99 -7.18 -26.34 -2.40   0.00 -12.01  0.00   0.00  19.24  12.00   0.00 -16.78   0.00 -38.42  -19.29   \n",
      "2   2  323    1063  49.85  0.0   0.00 -26.64 -23.66 -22.14  49.12  53.61  -4.70 -28.27   0.00 -33.01  6.30 -31.45 -3.15 -26.78  33.18 -9.45  40.98  -4.71 -17.26  25.14   4.71 -17.27  15.73  -18.85   \n",
      "\n",
      "    r26    r27    r28     r29    r30   r31   r32    r33   r34    r35  r36    r37    r38    r39   r40    r41    r42   r43    r44   r45    r46   r47   r48    r49    r50   r51    r52  \n",
      "0  0.00    NaN    NaN -102.39    NaN   NaN   NaN   0.00   NaN -34.36  NaN   0.00    NaN    NaN  0.00   0.00   0.00  0.00   0.00   NaN   0.00   NaN  0.00    NaN    NaN   NaN   0.00  \n",
      "1  0.00  14.49  14.46  -21.68 -45.87 -9.70  9.71  14.55  2.42   2.42  0.0   4.85   9.70  14.52  0.00  16.95   2.42 -4.83 -16.92 -4.84   4.84  0.00  7.26  -9.68 -19.38  9.71  26.68  \n",
      "2 -3.93   3.15  18.12    5.50   9.43  4.71  0.00  -1.57  0.78  10.99 -4.7 -20.41 -36.21 -38.73 -4.76  16.67 -68.08 -6.37   1.59  6.37 -49.32 -9.59 -6.40  22.41  -6.39  7.99  15.96  \n",
      "\n",
      "Head y_train:\n",
      "   ID  reod\n",
      "0   0     0\n",
      "1   1     0\n",
      "2   2    -1\n",
      "\n",
      "After merge on ID:\n",
      "  df_train: (843299, 57)\n",
      "\n",
      "Target distribution (reod):\n",
      "reod\n",
      "-1    0.300770\n",
      " 0    0.412031\n",
      " 1    0.287199\n",
      "Name: proportion, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, f1_score, classification_report\n",
    "\n",
    "import lightgbm as lgb\n",
    "\n",
    "# Display options \n",
    "pd.set_option(\"display.max_columns\", 80)\n",
    "pd.set_option(\"display.width\", 200)\n",
    "\n",
    "# File paths\n",
    "PATH_X_TRAIN = \"x_train.csv\"\n",
    "PATH_Y_TRAIN = \"y_train.csv\"\n",
    "PATH_X_TEST  = \"x_test.csv\"\n",
    "\n",
    "# Read CSVs\n",
    "X_train_raw = pd.read_csv(PATH_X_TRAIN)\n",
    "y_train_raw = pd.read_csv(PATH_Y_TRAIN)\n",
    "X_test_raw  = pd.read_csv(PATH_X_TEST)\n",
    "\n",
    "print(\"Shapes:\")\n",
    "print(\"  X_train:\", X_train_raw.shape)\n",
    "print(\"  y_train:\", y_train_raw.shape)\n",
    "print(\"  X_test :\", X_test_raw.shape)\n",
    "\n",
    "print(\"\\nHead X_train:\")\n",
    "print(X_train_raw.head(3))\n",
    "\n",
    "print(\"\\nHead y_train:\")\n",
    "print(y_train_raw.head(3))\n",
    "\n",
    "# Sanity: y_train must contain columns ['ID','reod']\n",
    "assert {\"ID\", \"reod\"}.issubset(y_train_raw.columns), \"y_train must contain columns ID and reod\"\n",
    "\n",
    "# Merge X_train and y_train on ID \n",
    "df_train = X_train_raw.merge(y_train_raw, on=\"ID\", how=\"inner\")\n",
    "print(\"\\nAfter merge on ID:\")\n",
    "print(\"  df_train:\", df_train.shape)\n",
    "\n",
    "# Basic class distribution\n",
    "print(\"\\nTarget distribution (reod):\")\n",
    "print(df_train[\"reod\"].value_counts(normalize=True).sort_index())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07ec5199",
   "metadata": {},
   "source": [
    "# Feature engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46684c11",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feature matrix shapes:\n",
      "  X_train_feats: (843299, 59)\n",
      "  X_test_feats : (885799, 59)\n",
      "\n",
      "Sample of engineered features:\n",
      "       r0      r1     r2     r3     r4     r5     r6     r7     r8     r9    r10    r11    r12    r13    r14    r15    r16    r17    r18    r19    r20    r21    r22    r23    r24     r25    r26  \\\n",
      "0    0.00    0.00   0.00   0.00   0.00   0.00   0.00 -68.03 -34.25   0.00   0.00   0.00   0.00   0.00   0.00   0.00   0.00   0.00   0.00   0.00   0.00   0.00   0.00   0.00   0.00  137.93   0.00   \n",
      "1   -9.76    0.00 -12.21  46.44  34.08   0.00  41.24  12.08 -26.54  19.32  48.22  23.99  -7.18 -26.34  -2.40   0.00 -12.01   0.00   0.00  19.24  12.00   0.00 -16.78   0.00 -38.42  -19.29   0.00   \n",
      "2   49.85    0.00   0.00 -26.64 -23.66 -22.14  49.12  53.61  -4.70 -28.27   0.00 -33.01   6.30 -31.45  -3.15 -26.78  33.18  -9.45  40.98  -4.71 -17.26  25.14   4.71 -17.27  15.73  -18.85  -3.93   \n",
      "3    0.00    0.00   0.00   0.00   0.00   0.00   0.00   0.00   0.00   0.00   0.00   0.00   0.00   0.00   0.00  11.59   0.00   0.00   0.00   0.00   0.00   0.00   0.00   0.00   0.00    0.00   0.00   \n",
      "4 -123.84 -115.18 -26.44   0.00  42.42  10.56   0.00 -47.57  21.28 -10.63 -31.91  21.32 -53.19 -32.05  21.46 -32.12 -53.65  10.79  43.10   0.00 -32.22 -10.76  48.47  21.44 -42.83   42.96  10.71   \n",
      "\n",
      "     r27    r28     r29    r30   r31    r32    r33    r34    r35    r36    r37    r38    r39    r40    r41    r42   r43    r44    r45    r46    r47   r48    r49    r50    r51    r52  ret_sum  \\\n",
      "0   0.00   0.00 -102.39   0.00  0.00   0.00   0.00   0.00 -34.36   0.00   0.00   0.00   0.00   0.00   0.00   0.00  0.00   0.00   0.00   0.00   0.00  0.00   0.00   0.00   0.00   0.00  -101.10   \n",
      "1  14.49  14.46  -21.68 -45.87 -9.70   9.71  14.55   2.42   2.42   0.00   4.85   9.70  14.52   0.00  16.95   2.42 -4.83 -16.92  -4.84   4.84   0.00  7.26  -9.68 -19.38   9.71  26.68   107.76   \n",
      "2   3.15  18.12    5.50   9.43  4.71   0.00  -1.57   0.78  10.99  -4.70 -20.41 -36.21 -38.73  -4.76  16.67 -68.08 -6.37   1.59   6.37 -49.32  -9.59 -6.40  22.41  -6.39   7.99  15.96  -121.51   \n",
      "3   0.00   0.00    0.00   0.00  0.00   0.00   0.00   0.00   0.00   0.00   0.00   0.00   0.00   0.00   0.00   0.00  0.00   0.00   0.00   0.00   0.00  0.00   0.00   0.00   0.00   0.00    11.59   \n",
      "4 -10.70  16.06   10.70 -10.68  0.00 -26.74 -37.53  32.29 -10.73 -26.88  26.94 -21.48  64.52  26.70 -42.55 -21.39 -5.36 -21.44 -21.48  10.78 -21.55 -5.40 -10.81   5.41 -32.47  43.43  -408.24   \n",
      "\n",
      "     ret_std  pos_count  neg_count  big_move_count_25  big_move_count_50  \n",
      "0  26.420162          1          4                  5                  3  \n",
      "1  18.820373         24         18                  9                  0  \n",
      "2  24.011006         22         27                 15                  2  \n",
      "3   1.592009          1          0                  0                  0  \n",
      "4  36.138157         20         29                 25                  5  \n"
     ]
    }
   ],
   "source": [
    "def build_features(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Create model-ready features from raw intraday returns.\n",
    "    Here we go for a *simple but strong* representation:\n",
    "    - raw r0..r52\n",
    "    - a few global stats (sum, std, counts)\n",
    "    \"\"\"\n",
    "    df = df.copy()\n",
    "\n",
    "    # List of return columns r0..r52, ordered\n",
    "    ret_cols = [col for col in df.columns if col.startswith(\"r\")]\n",
    "    ret_cols = sorted(ret_cols, key=lambda x: int(x[1:]))\n",
    "\n",
    "    # Cast returns to float and fill NaN with 0\n",
    "    df[ret_cols] = df[ret_cols].astype(float).fillna(0.0)\n",
    "\n",
    "    # Global stats on the path \n",
    "    df[\"ret_sum\"] = df[ret_cols].sum(axis=1)\n",
    "    df[\"ret_std\"] = df[ret_cols].std(axis=1)\n",
    "    df[\"pos_count\"] = (df[ret_cols] > 0).sum(axis=1)\n",
    "    df[\"neg_count\"] = (df[ret_cols] < 0).sum(axis=1)\n",
    "    df[\"big_move_count_25\"] = (df[ret_cols].abs() > 25).sum(axis=1)\n",
    "    df[\"big_move_count_50\"] = (df[ret_cols].abs() > 50).sum(axis=1)\n",
    "\n",
    "    # We do NOT include 'day' or 'equity' here to avoid overfitting\n",
    "    # on IDs that do not appear in the test set.\n",
    "\n",
    "    feature_cols = ret_cols + [\n",
    "        \"ret_sum\",\n",
    "        \"ret_std\",\n",
    "        \"pos_count\",\n",
    "        \"neg_count\",\n",
    "        \"big_move_count_25\",\n",
    "        \"big_move_count_50\",\n",
    "    ]\n",
    "\n",
    "    # Robustness: only keep cols that exist in the dataframe\n",
    "    feature_cols = [c for c in feature_cols if c in df.columns]\n",
    "\n",
    "    return df[feature_cols], ret_cols\n",
    "\n",
    "\n",
    "# Build features for train and test \n",
    "\n",
    "X_train_feats, ret_cols_train = build_features(X_train_raw)\n",
    "X_test_feats, ret_cols_test = build_features(X_test_raw)\n",
    "\n",
    "# Sanity: we ensure same feature columns order between train and test\n",
    "assert list(X_train_feats.columns) == list(X_test_feats.columns), \\\n",
    "    \"Train and test feature columns mismatch\"\n",
    "\n",
    "print(\"Feature matrix shapes:\")\n",
    "print(\"  X_train_feats:\", X_train_feats.shape)\n",
    "print(\"  X_test_feats :\", X_test_feats.shape)\n",
    "\n",
    "print(\"\\nSample of engineered features:\")\n",
    "print(X_train_feats.head(5))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51dfc397",
   "metadata": {},
   "source": [
    "# Train/validation split & LightGBM training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af1682ed",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shapes after split:\n",
      "  X_tr : (674639, 59)\n",
      "  X_val: (168660, 59)\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.336526 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 14207\n",
      "[LightGBM] [Info] Number of data points in the train set: 674639, number of used features: 59\n",
      "[LightGBM] [Info] Start training from score -1.201410\n",
      "[LightGBM] [Info] Start training from score -0.886657\n",
      "[LightGBM] [Info] Start training from score -1.247578\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "[100]\ttrain's multi_logloss: 0.997587\ttrain's multi_error: 0.507829\tvalid's multi_logloss: 1.0019\tvalid's multi_error: 0.515688\n",
      "[200]\ttrain's multi_logloss: 0.988922\ttrain's multi_error: 0.49589\tvalid's multi_logloss: 0.997918\tvalid's multi_error: 0.511443\n",
      "[300]\ttrain's multi_logloss: 0.982569\ttrain's multi_error: 0.486273\tvalid's multi_logloss: 0.996104\tvalid's multi_error: 0.510441\n",
      "[400]\ttrain's multi_logloss: 0.976964\ttrain's multi_error: 0.477387\tvalid's multi_logloss: 0.99491\tvalid's multi_error: 0.509303\n",
      "[500]\ttrain's multi_logloss: 0.971777\ttrain's multi_error: 0.469675\tvalid's multi_logloss: 0.994009\tvalid's multi_error: 0.508289\n",
      "[600]\ttrain's multi_logloss: 0.966669\ttrain's multi_error: 0.46259\tvalid's multi_logloss: 0.993217\tvalid's multi_error: 0.506818\n",
      "[700]\ttrain's multi_logloss: 0.961646\ttrain's multi_error: 0.455663\tvalid's multi_logloss: 0.992458\tvalid's multi_error: 0.506789\n",
      "[800]\ttrain's multi_logloss: 0.956854\ttrain's multi_error: 0.449466\tvalid's multi_logloss: 0.991725\tvalid's multi_error: 0.505538\n",
      "[900]\ttrain's multi_logloss: 0.952129\ttrain's multi_error: 0.443305\tvalid's multi_logloss: 0.991065\tvalid's multi_error: 0.505016\n",
      "[1000]\ttrain's multi_logloss: 0.947459\ttrain's multi_error: 0.437769\tvalid's multi_logloss: 0.99046\tvalid's multi_error: 0.503955\n",
      "[1100]\ttrain's multi_logloss: 0.943062\ttrain's multi_error: 0.432222\tvalid's multi_logloss: 0.990007\tvalid's multi_error: 0.504239\n",
      "[1200]\ttrain's multi_logloss: 0.938612\ttrain's multi_error: 0.426532\tvalid's multi_logloss: 0.989578\tvalid's multi_error: 0.503783\n",
      "[1300]\ttrain's multi_logloss: 0.934246\ttrain's multi_error: 0.42117\tvalid's multi_logloss: 0.989198\tvalid's multi_error: 0.503741\n",
      "[1400]\ttrain's multi_logloss: 0.929999\ttrain's multi_error: 0.416048\tvalid's multi_logloss: 0.988851\tvalid's multi_error: 0.502882\n",
      "[1500]\ttrain's multi_logloss: 0.925787\ttrain's multi_error: 0.410921\tvalid's multi_logloss: 0.988538\tvalid's multi_error: 0.50281\n",
      "[1600]\ttrain's multi_logloss: 0.921629\ttrain's multi_error: 0.405784\tvalid's multi_logloss: 0.988229\tvalid's multi_error: 0.502893\n",
      "[1700]\ttrain's multi_logloss: 0.917483\ttrain's multi_error: 0.400939\tvalid's multi_logloss: 0.987944\tvalid's multi_error: 0.503136\n",
      "Early stopping, best iteration is:\n",
      "[1572]\ttrain's multi_logloss: 0.922791\ttrain's multi_error: 0.407265\tvalid's multi_logloss: 0.988321\tvalid's multi_error: 0.502544\n",
      "\n",
      "Best iteration (early stopping): 1572\n",
      "\n",
      "Validation accuracy : 0.4975\n",
      "Validation F1-macro : 0.4591\n",
      "\n",
      "Classification report (validation):\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "          -1     0.4434    0.3775    0.4078     50728\n",
      "           0     0.5382    0.7326    0.6205     69493\n",
      "           1     0.4482    0.2858    0.3490     48439\n",
      "\n",
      "    accuracy                         0.4975    168660\n",
      "   macro avg     0.4766    0.4653    0.4591    168660\n",
      "weighted avg     0.4838    0.4975    0.4786    168660\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Target\n",
    "y = df_train[\"reod\"].values\n",
    "\n",
    "# Map target {-1,0,1} -> {0,1,2} for LightGBM\n",
    "label_mapping = {-1: 0, 0: 1, 1: 2}\n",
    "inv_label_mapping = {v: k for k, v in label_mapping.items()}\n",
    "y_mapped = np.array([label_mapping[val] for val in y])\n",
    "\n",
    "# Align features with df_train by index (ID is already aligned by construction)\n",
    "X = X_train_feats.values\n",
    "\n",
    "# Train/validation split (stratified)\n",
    "X_tr, X_val, y_tr, y_val = train_test_split(\n",
    "    X,\n",
    "    y_mapped,\n",
    "    test_size=0.2,\n",
    "    random_state=42,\n",
    "    stratify=y_mapped,\n",
    ")\n",
    "\n",
    "print(\"Shapes after split:\")\n",
    "print(\"  X_tr :\", X_tr.shape)\n",
    "print(\"  X_val:\", X_val.shape)\n",
    "\n",
    "# LightGBM parameters (cell to modify if tuning but not very significative)\n",
    "lgb_params = {\n",
    "    \"objective\": \"multiclass\",\n",
    "    \"num_class\": 3,\n",
    "    \"learning_rate\": 0.05,\n",
    "    \"num_leaves\": 31,          \n",
    "    \"max_depth\": -1,\n",
    "    \"feature_fraction\": 0.9,\n",
    "    \"bagging_fraction\": 0.8,\n",
    "    \"bagging_freq\": 1,\n",
    "    \"min_data_in_leaf\": 200,   \n",
    "    \"lambda_l2\": 2.0,\n",
    "    \"metric\": [\"multi_logloss\", \"multi_error\"],  \n",
    "    \"n_estimators\": 2000,\n",
    "    \"random_state\": 42,\n",
    "    \"n_jobs\": -1,\n",
    "}\n",
    "\n",
    "# Dataset objects for LightGBM\n",
    "train_set = lgb.Dataset(X_tr, label=y_tr)\n",
    "valid_set = lgb.Dataset(X_val, label=y_val, reference=train_set)\n",
    "\n",
    "# Training with early stopping\n",
    "evals_result = {}\n",
    "gbm = lgb.train(\n",
    "    params=lgb_params,\n",
    "    train_set=train_set,\n",
    "    valid_sets=[train_set, valid_set],\n",
    "    valid_names=[\"train\", \"valid\"],\n",
    "    num_boost_round=lgb_params[\"n_estimators\"],\n",
    "    callbacks=[\n",
    "        lgb.early_stopping(stopping_rounds=200),\n",
    "        lgb.log_evaluation(period=100),\n",
    "        lgb.record_evaluation(evals_result),\n",
    "    ],\n",
    ")\n",
    "\n",
    "best_iter = gbm.best_iteration\n",
    "print(f\"\\nBest iteration (early stopping): {best_iter}\")\n",
    "\n",
    "# Validation metrics\n",
    "y_val_pred_prob = gbm.predict(X_val, num_iteration=best_iter)\n",
    "y_val_pred = np.argmax(y_val_pred_prob, axis=1)\n",
    "\n",
    "# Map back to {-1,0,1}\n",
    "y_val_true_labels = np.array([inv_label_mapping[i] for i in y_val])\n",
    "y_val_pred_labels = np.array([inv_label_mapping[i] for i in y_val_pred])\n",
    "\n",
    "acc = accuracy_score(y_val_true_labels, y_val_pred_labels)\n",
    "f1_macro = f1_score(y_val_true_labels, y_val_pred_labels, average=\"macro\")\n",
    "\n",
    "print(f\"\\nValidation accuracy : {acc:.4f}\")\n",
    "print(f\"Validation F1-macro : {f1_macro:.4f}\")\n",
    "\n",
    "print(\"\\nClassification report (validation):\")\n",
    "print(classification_report(y_val_true_labels, y_val_pred_labels, digits=4))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "afcc89ba",
   "metadata": {},
   "source": [
    "# Full training & test predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f1aae58",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.181750 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 14208\n",
      "[LightGBM] [Info] Number of data points in the train set: 843299, number of used features: 59\n",
      "[LightGBM] [Info] Start training from score -1.201410\n",
      "[LightGBM] [Info] Start training from score -0.886658\n",
      "[LightGBM] [Info] Start training from score -1.247578\n",
      "        ID  reod\n",
      "0  1000000     1\n",
      "1  1000001     1\n",
      "2  1000002     1\n",
      "3  1000003     1\n",
      "4  1000004     0\n",
      "reod\n",
      "-1    0.389654\n",
      " 1    0.309911\n",
      " 0    0.300435\n",
      "Name: proportion, dtype: float64\n",
      "\n",
      ">> y_prediction.csv created\n"
     ]
    }
   ],
   "source": [
    "# We reuse:\n",
    "# - X_train_feats, X_test_feats\n",
    "# - y_mapped, label_mapping, inv_label_mapping\n",
    "# - lgb_params\n",
    "# - best_iter\n",
    "\n",
    "X_full = X_train_feats.values\n",
    "y_full = y_mapped\n",
    "\n",
    "# We fix n_estimators to the best_iter found on validation\n",
    "lgb_params_final = lgb_params.copy()\n",
    "lgb_params_final[\"n_estimators\"] = best_iter\n",
    "\n",
    "train_set_full = lgb.Dataset(X_full, label=y_full)\n",
    "\n",
    "gbm_full = lgb.train(\n",
    "    params=lgb_params_final,\n",
    "    train_set=train_set_full,\n",
    "    num_boost_round=best_iter\n",
    ")\n",
    "\n",
    "# Predictions on the test set\n",
    "X_test_mat = X_test_feats.values\n",
    "y_test_pred_prob = gbm_full.predict(X_test_mat, num_iteration=best_iter)\n",
    "y_test_pred = np.argmax(y_test_pred_prob, axis=1)\n",
    "\n",
    "# Remap to {-1, 0, 1}\n",
    "y_test_pred_labels = np.array([inv_label_mapping[i] for i in y_test_pred])\n",
    "\n",
    "# Build y_prediction.csv\n",
    "submission = pd.DataFrame({\n",
    "    \"ID\": X_test_raw[\"ID\"].values,\n",
    "    \"reod\": y_test_pred_labels,\n",
    "})\n",
    "\n",
    "print(submission.head())\n",
    "print(submission[\"reod\"].value_counts(normalize=True))\n",
    "\n",
    "submission.to_csv(\"y_prediction.csv\", index=False)\n",
    "print(\"\\n>> y_prediction.csv created\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "telecom_env312",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
