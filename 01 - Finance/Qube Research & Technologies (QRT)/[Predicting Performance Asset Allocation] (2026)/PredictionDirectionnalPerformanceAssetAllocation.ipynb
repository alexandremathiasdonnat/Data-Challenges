{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "525a3b03",
   "metadata": {},
   "source": [
    "<p align=\"center\">\n",
    "    <img src=\"https://upload.wikimedia.org/wikipedia/commons/7/74/Logo_%C3%89cole_normale_sup%C3%A9rieure_-_PSL_%28ENS-PSL%29.svg\"\n",
    "             alt=\"ENS-PSL\"\n",
    "             width=\"500\"\n",
    "             style=\"margin-right: 30px; display: inline-block; vertical-align: middle;\"/>\n",
    "    <img src=\"https://upload.wikimedia.org/wikipedia/en/3/3f/Qube_Research_%26_Technologies_Logo.svg\"\n",
    "             alt=\"QRT\"\n",
    "             width=\"200\"\n",
    "             style=\"display: inline-block; vertical-align: middle;\"/>\n",
    "</p>\n",
    "\n",
    "\n",
    "# QRT - Predicting Directional Performance of Asset Allocations \n",
    "**Meta-Decision Learning on Systematic Portfolio Signals**\n",
    "\n",
    "## Data Challenge  \n",
    "**Powered by ENS**\n",
    "\n",
    "<h3><span style=\"color:#800000;\"><strong>Authored by:</strong> <em>Alexandre Mathias DONNAT, Sr</em></span></h3>\n",
    "\n",
    "**Currently ranked 79/355:** on *https://challengedata.ens.fr/challenges/167*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "655ccc28",
   "metadata": {},
   "source": [
    "This notebook tackles a meta-decision problem in systematic trading:\n",
    "\n",
    "Given the recent behaviour of a portfolio allocation, should we trust it (follow its weights), or fade it (take the opposite position)?\n",
    "\n",
    "Each row of the dataset represents a systematic allocation of assets, defined by:\n",
    "\n",
    "- 20 past daily returns\n",
    "- 20 past signed liquidity measures\n",
    "- its median daily turnover\n",
    "- its group identifier\n",
    "- and its future return (target)\n",
    "\n",
    "The goal is not to predict the magnitude of the return, but only its sign.\n",
    "\n",
    "Formally, we solve a binary directional classification problem under a time-series constraint, optimising the official metric:\n",
    "\n",
    "$$\\text{Accuracy} = \\frac{1}{N} \\sum_{i=1}^{N} \\mathbb{1}\\left[\\text{sign}(\\hat{r}_i) = \\text{sign}(r_i)\\right]$$\n",
    "\n",
    "This is a decision problem, not a regression problem.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab75189a",
   "metadata": {},
   "source": [
    "# 1. Mathematical Formulation\n",
    "\n",
    "## 1.1 Allocation structure\n",
    "\n",
    "For an allocation $S$ at date $t$, with $M$ assets:\n",
    "\n",
    "$$r_{S,t+1} = \\sum_{i=1}^{M} w_{S,t,i} \\cdot r_{i,t+1}$$\n",
    "\n",
    "where:\n",
    "\n",
    "- $w_{S,t,i}$ are portfolio weights satisfying: $\\sum_{i=1}^{M} |w_{S,t,i}| = 1$\n",
    "- $r_{i,t+1}$ is the return of asset $i$ on the next session.\n",
    "\n",
    "Each row in the dataset gives:\n",
    "\n",
    "$$\\{r_{S,t}, r_{S,t-1}, \\ldots, r_{S,t-19}\\}$$\n",
    "\n",
    "plus liquidity proxies and turnover summaries.\n",
    "\n",
    "Our objective is to learn a function:\n",
    "\n",
    "$$f: \\mathbb{R}^d \\to \\{0, 1\\}$$\n",
    "\n",
    "such that:\n",
    "\n",
    "- $1$ → trust allocation\n",
    "- $0$ → fade allocation\n",
    "\n",
    "## 1.2 Nature of the metric\n",
    "\n",
    "The challenge metric only evaluates directional correctness.\n",
    "\n",
    "- Predicting $+0.0001$ or $+0.10$ is identical.\n",
    "- Magnitude is irrelevant.\n",
    "- Only the sign matters.\n",
    "\n",
    "This has important consequences:\n",
    "\n",
    "- MSE optimisation is suboptimal.\n",
    "- Calibration matters less than directional stability.\n",
    "- Overfitting noise in magnitude does not improve score.\n",
    "\n",
    "The problem becomes a binary classification under heavy noise, with:\n",
    "\n",
    "- weak signal-to-noise ratio,\n",
    "- temporal regime shifts,\n",
    "- heterogeneity across allocation groups.\n",
    "\n",
    "# 2. Data Description\n",
    "\n",
    "## 2.1 Training set\n",
    "\n",
    "The dataset consists of:\n",
    "\n",
    "- 527,073 observations in training,\n",
    "- 31,870 observations in test.\n",
    "\n",
    "Each row corresponds to a tuple: $(TS, ALLOCATION)$\n",
    "\n",
    "Columns include:\n",
    "\n",
    "- `RET_1` to `RET_20` — past daily allocation returns,\n",
    "- `SIGNED_VOLUME_1` to `SIGNED_VOLUME_20`,\n",
    "- `MEDIAN_DAILY_TURNOVER`,\n",
    "- `GROUP`,\n",
    "- `TARGET` (future allocation return).\n",
    "\n",
    "The target is transformed into:\n",
    "\n",
    "$$y = \\mathbb{1}[TARGET > 0]$$\n",
    "\n",
    "## 2.2 Time-series structure\n",
    "\n",
    "The dataset is indexed by:\n",
    "\n",
    "- anonymised timestamps (`TS`)\n",
    "- allocation identifiers\n",
    "\n",
    "Important:\n",
    "\n",
    "- `TS` labels are ordered but not guaranteed continuous.\n",
    "- No shuffling is allowed.\n",
    "- Validation must respect temporal ordering.\n",
    "\n",
    "All splits are therefore performed via:\n",
    "\n",
    "- train = first 80% TS\n",
    "- validation = last 20% TS\n",
    "\n",
    "This ensures strict leakage-free evaluation.\n",
    "\n",
    "# 3. Feature Engineering Philosophy\n",
    "\n",
    "The approach used in this notebook is deliberately controlled.\n",
    "\n",
    "We avoid:\n",
    "\n",
    "- extreme feature explosion,\n",
    "- over-engineered statistical transformations,\n",
    "- cross-sectional leakage tricks.\n",
    "\n",
    "Instead, we construct:\n",
    "\n",
    "## 3.1 Rolling return descriptors\n",
    "\n",
    "For windows $k \\in \\{3, 5, 10, 20\\}$:\n",
    "\n",
    "- mean return\n",
    "- volatility\n",
    "- min / max\n",
    "- sign fraction\n",
    "- entropy of sign distribution\n",
    "- linear slope (trend proxy)\n",
    "\n",
    "These capture:\n",
    "\n",
    "- short-term momentum,\n",
    "- stability,\n",
    "- directional persistence,\n",
    "- trend strength.\n",
    "\n",
    "## 3.2 Scale-free normalisations\n",
    "\n",
    "We introduce:\n",
    "\n",
    "$$\\text{ret\\_norm}_1 = \\frac{RET_1}{\\sigma_5}$$\n",
    "\n",
    "This removes volatility scale and improves directional comparability across allocations.\n",
    "\n",
    "## 3.3 Liquidity behaviour\n",
    "\n",
    "Analogous rolling statistics are computed on signed volumes:\n",
    "\n",
    "- mean\n",
    "- volatility\n",
    "- extrema\n",
    "\n",
    "Liquidity interacts with return stability and turnover.\n",
    "\n",
    "## 3.4 Limited interactions\n",
    "\n",
    "We include only a few economically motivated interactions:\n",
    "\n",
    "- momentum × turnover\n",
    "- normalized return × entropy\n",
    "\n",
    "We deliberately avoid high-order combinatorics.\n",
    "\n",
    "# 4. Model\n",
    "\n",
    "We use LightGBM with:\n",
    "\n",
    "- binary objective,\n",
    "- early stopping,\n",
    "- categorical handling for: `GROUP`, `ALLOCATION`, `TS`\n",
    "\n",
    "Key hyperparameters:\n",
    "\n",
    "- `learning_rate = 0.03`\n",
    "- `num_leaves = 128`\n",
    "- `min_data_in_leaf = 200`\n",
    "- `feature_fraction = 0.8`\n",
    "- `bagging_fraction = 0.8`\n",
    "- L2 regularisation\n",
    "\n",
    "The model is trained on the first 80% of timestamps and validated on the last 20%.\n",
    "\n",
    "The final model is refit on the full training data using the selected number of boosting rounds.\n",
    "\n",
    "# 5. Results\n",
    "\n",
    "Best public score achieved: $0.5154$\n",
    "\n",
    "Validation accuracy: $\\approx 0.526$\n",
    "\n",
    "This indicates:\n",
    "\n",
    "- real predictive signal,\n",
    "- controlled generalisation gap,\n",
    "- absence of extreme overfitting.\n",
    "\n",
    "Extensive attempts at feature inflation (tail risk, cross-sectional ranks, allocation priors, regime calibration) did not improve public performance, suggesting that the current representation is close to the signal frontier under this modelling class.\n",
    "\n",
    "# 6. Interpretation\n",
    "\n",
    "This challenge illustrates a key principle:\n",
    "\n",
    "A good data challenge is rarely won by a more complex model.\n",
    "It is won by a better formulation.\n",
    "\n",
    "The core signal here lies in:\n",
    "\n",
    "- recent directional stability,\n",
    "- short-term volatility scaling,\n",
    "- regime heterogeneity encoded by `TS` and `GROUP`.\n",
    "\n",
    "Beyond a certain complexity threshold, additional features introduce variance rather than signal.\n",
    "\n",
    "# 7. Code Pipeline\n",
    "\n",
    "The following sections implement:\n",
    "\n",
    "- Data loading and preprocessing\n",
    "- Feature engineering\n",
    "- Time-based split\n",
    "- LightGBM training with early stopping\n",
    "- Final refit and submission generation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "877a5970",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(527073, 47) (31870, 45)\n",
      "Baseline always-long: 0.5071840143585423\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import lightgbm as lgb\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "X_train = pd.read_csv(\"X_train.csv\")\n",
    "y_train = pd.read_csv(\"y_train.csv\")\n",
    "X_test  = pd.read_csv(\"X_test.csv\")\n",
    "\n",
    "df = X_train.merge(y_train, on=\"ROW_ID\")\n",
    "df[\"y\"] = (df[\"target\"] > 0).astype(int)\n",
    "\n",
    "df = df.sort_values([\"TS\", \"ALLOCATION\"]).reset_index(drop=True)\n",
    "\n",
    "ret_cols = [f\"RET_{i}\" for i in range(1, 21)]\n",
    "vol_cols = [f\"SIGNED_VOLUME_{i}\" for i in range(1, 21)]\n",
    "\n",
    "print(df.shape, X_test.shape)\n",
    "print(\"Baseline always-long:\", df[\"y\"].mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "697ef132",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Features added. Train columns: 97 Test columns: 95\n"
     ]
    }
   ],
   "source": [
    "def sign_entropy(p: pd.Series) -> pd.Series:\n",
    "    eps = 1e-9\n",
    "    return -(p*np.log(p+eps) + (1-p)*np.log(1-p+eps))\n",
    "\n",
    "def add_features(frame: pd.DataFrame) -> pd.DataFrame:\n",
    "    f = frame.copy()\n",
    "\n",
    "    for k in [3, 5, 10, 20]:\n",
    "        cols = [f\"RET_{i}\" for i in range(1, k+1)]\n",
    "        f[f\"ret_mean_{k}\"] = f[cols].mean(axis=1)\n",
    "        f[f\"ret_std_{k}\"]  = f[cols].std(axis=1)\n",
    "        f[f\"ret_min_{k}\"]  = f[cols].min(axis=1)\n",
    "        f[f\"ret_max_{k}\"]  = f[cols].max(axis=1)\n",
    "        f[f\"sign_frac_{k}\"] = (f[cols] > 0).mean(axis=1)\n",
    "        f[f\"sign_entropy_{k}\"] = sign_entropy(f[f\"sign_frac_{k}\"])\n",
    "\n",
    "        # slope proxy \n",
    "        t = np.arange(k)\n",
    "        # fast closed-form slope: cov(t,x)/var(t)\n",
    "        denom = ((t - t.mean())**2).sum()\n",
    "        centered_t = (t - t.mean())\n",
    "        Xk = f[cols].to_numpy()\n",
    "        slope = (Xk @ centered_t) / denom\n",
    "        f[f\"ret_slope_{k}\"] = slope\n",
    "\n",
    "    # Momentum + scale-free versions\n",
    "    f[\"vol_5\"] = f[[f\"RET_{i}\" for i in range(1, 6)]].std(axis=1) + 1e-6\n",
    "    f[\"ret_norm_1\"] = f[\"RET_1\"] / f[\"vol_5\"]\n",
    "    f[\"ret_norm_3\"] = f[\"RET_1\"] / (f[[f\"RET_{i}\" for i in range(1, 4)]].std(axis=1) + 1e-6)\n",
    "    f[\"ret_norm_10\"] = f[\"RET_1\"] / (f[[f\"RET_{i}\" for i in range(1, 11)]].std(axis=1) + 1e-6)\n",
    "\n",
    "    # Signed volumes: same family of stats\n",
    "    for k in [3, 5, 10, 20]:\n",
    "        cols = [f\"SIGNED_VOLUME_{i}\" for i in range(1, k+1)]\n",
    "        f[f\"vol_mean_{k}\"] = f[cols].mean(axis=1)\n",
    "        f[f\"vol_std_{k}\"]  = f[cols].std(axis=1)\n",
    "        f[f\"vol_min_{k}\"]  = f[cols].min(axis=1)\n",
    "        f[f\"vol_max_{k}\"]  = f[cols].max(axis=1)\n",
    "\n",
    "    # Interactions (few, not insane)\n",
    "    f[\"mom5_x_turn\"] = f[\"ret_mean_5\"] * f[\"MEDIAN_DAILY_TURNOVER\"].astype(float)\n",
    "    f[\"norm1_x_entropy5\"] = f[\"ret_norm_1\"] * f[\"sign_entropy_5\"]\n",
    "\n",
    "    # Categorical encodings (simple)\n",
    "    f[\"GROUP\"] = f[\"GROUP\"].astype(\"category\")\n",
    "    f[\"ALLOCATION\"] = f[\"ALLOCATION\"].astype(\"category\")\n",
    "    f[\"TS\"] = f[\"TS\"].astype(\"category\")\n",
    "\n",
    "    return f\n",
    "\n",
    "df_feat = add_features(df)\n",
    "test_feat = add_features(X_test)\n",
    "\n",
    "print(\"Features added. Train columns:\", df_feat.shape[1], \"Test columns:\", test_feat.shape[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81de08f2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training until validation scores don't improve for 200 rounds\n",
      "[200]\ttrain's binary_logloss: 0.576813\tvalid's binary_logloss: 0.691339\n",
      "Early stopping, best iteration is:\n",
      "[46]\ttrain's binary_logloss: 0.651171\tvalid's binary_logloss: 0.690901\n",
      "VAL accuracy: 0.526022 | delta vs always-long: +0.009715 | best_iter=46\n"
     ]
    }
   ],
   "source": [
    "# Time-based split by TS\n",
    "ts_unique = df_feat[\"TS\"].cat.categories.tolist() if str(df_feat[\"TS\"].dtype) == \"category\" else sorted(df_feat[\"TS\"].unique())\n",
    "cut = int(0.8 * len(ts_unique))\n",
    "ts_train = set(ts_unique[:cut])\n",
    "ts_val   = set(ts_unique[cut:])\n",
    "\n",
    "train_mask = df_feat[\"TS\"].isin(ts_train)\n",
    "val_mask   = df_feat[\"TS\"].isin(ts_val)\n",
    "\n",
    "# Feature set: drop raw identifiers + target columns, keep engineered + base lags\n",
    "drop_cols = {\"ROW_ID\", \"target\", \"y\"}  \n",
    "# keep raw lags too (LGBM may exploit non-linear patterns)\n",
    "feature_cols = [c for c in df_feat.columns if c not in drop_cols]\n",
    "\n",
    "X_tr = df_feat.loc[train_mask, feature_cols]\n",
    "y_tr = df_feat.loc[train_mask, \"y\"]\n",
    "X_va = df_feat.loc[val_mask, feature_cols]\n",
    "y_va = df_feat.loc[val_mask, \"y\"]\n",
    "\n",
    "# LightGBM datasets\n",
    "dtrain = lgb.Dataset(X_tr, label=y_tr, categorical_feature=[\"GROUP\", \"ALLOCATION\", \"TS\"], free_raw_data=False)\n",
    "dvalid = lgb.Dataset(X_va, label=y_va, categorical_feature=[\"GROUP\", \"ALLOCATION\", \"TS\"], free_raw_data=False)\n",
    "\n",
    "params = dict(\n",
    "    objective=\"binary\",\n",
    "    metric=\"binary_logloss\",\n",
    "    learning_rate=0.03,\n",
    "    num_leaves=128,\n",
    "    max_depth=-1,\n",
    "    min_data_in_leaf=200,\n",
    "    feature_fraction=0.8,\n",
    "    bagging_fraction=0.8,\n",
    "    bagging_freq=1,\n",
    "    lambda_l2=5.0,\n",
    "    lambda_l1=0.0,\n",
    "    min_gain_to_split=0.0,\n",
    "    verbose=-1,\n",
    ")\n",
    "\n",
    "model = lgb.train(\n",
    "    params,\n",
    "    dtrain,\n",
    "    num_boost_round=5000,\n",
    "    valid_sets=[dtrain, dvalid],\n",
    "    valid_names=[\"train\", \"valid\"],\n",
    "    callbacks=[\n",
    "        lgb.early_stopping(stopping_rounds=200),\n",
    "        lgb.log_evaluation(period=200),\n",
    "    ],\n",
    ")\n",
    "\n",
    "# Evaluate\n",
    "va_proba = model.predict(X_va, num_iteration=model.best_iteration)\n",
    "va_pred = (va_proba > 0.5).astype(int)\n",
    "acc_val = accuracy_score(y_va, va_pred)\n",
    "baseline_val = y_va.mean() \n",
    "\n",
    "print(f\"VAL accuracy: {acc_val:.6f} | delta vs always-long: {acc_val - baseline_val:+.6f} | best_iter={model.best_iteration}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a240b7a7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   ROW_ID  prediction\n",
      "0  527073           1\n",
      "1  527074           1\n",
      "2  527075           0\n",
      "3  527076           0\n",
      "4  527077           0\n",
      "Pred mean: 0.613680577345466\n"
     ]
    }
   ],
   "source": [
    "# Refit on full data using best_iteration\n",
    "X_full = df_feat[feature_cols]\n",
    "y_full = df_feat[\"y\"]\n",
    "\n",
    "dfull = lgb.Dataset(X_full, label=y_full, categorical_feature=[\"GROUP\", \"ALLOCATION\", \"TS\"], free_raw_data=False)\n",
    "\n",
    "final_model = lgb.train(\n",
    "    params,\n",
    "    dfull,\n",
    "    num_boost_round=model.best_iteration,  # lock the complexity\n",
    "    valid_sets=[dfull],\n",
    "    valid_names=[\"train\"],\n",
    "    callbacks=[lgb.log_evaluation(period=300)],\n",
    ")\n",
    "\n",
    "X_te = test_feat[feature_cols]\n",
    "te_proba = final_model.predict(X_te)\n",
    "te_pred = (te_proba > 0.5).astype(int)\n",
    "\n",
    "submission = pd.DataFrame({\n",
    "    \"ROW_ID\": X_test[\"ROW_ID\"],\n",
    "    \"prediction\": te_pred\n",
    "})\n",
    "submission.to_csv(\"submission_lgbm.csv\", index=False)\n",
    "\n",
    "print(submission.head())\n",
    "print(\"Pred mean:\", te_pred.mean())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "telecom_env312",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
